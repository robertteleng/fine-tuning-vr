# Fine-Tuning YOLO12 para DetecciÃ³n de Pilares VR

## Objetivo del Proyecto

Modelo de detecciÃ³n de objetos especializado en identificar **pilares de seÃ±alizaciÃ³n amarillo/negro** en entornos de Realidad Virtual. El modelo estÃ¡ optimizado para tiempo real usando YOLOv12s.

## Pipeline del Proyecto

```mermaid
flowchart LR
    subgraph Datos
        A[ðŸŽ¥ Video VR] --> B[ðŸ“¸ Extraer Frames]
        B --> C[ðŸ·ï¸ Anotar]
    end

    subgraph Entrenamiento
        C --> D[ðŸ§  Fine-tune YOLO12s]
        D --> E[ðŸ“Š Evaluar MÃ©tricas]
    end

    subgraph ProducciÃ³n
        E --> F[âš¡ Exportar TensorRT]
        F --> G[ðŸš€ Inferencia 180 FPS]
    end

    style A fill:#e1f5fe
    style G fill:#c8e6c9
```

## Resultados Actuales

| MÃ©trica | Valor |
|---------|-------|
| mAP@50 | 98.7% |
| mAP@50-95 | 87.4% |
| PrecisiÃ³n | 91.7% |
| Recall | 99.2% |
| Inferencia PyTorch | 14ms/imagen (71 FPS) |
| **Inferencia TensorRT FP16** | **5.5ms/imagen (180 FPS)** |

---

## Hardware

| Componente | EspecificaciÃ³n |
|------------|----------------|
| GPU | NVIDIA RTX 2060 (6GB VRAM) |
| CUDA | 13.0 |
| Driver | 580.95.05 |

---

## Estructura del Proyecto

```
fine-tuning-vr/
â”œâ”€â”€ config.yaml                  # ConfiguraciÃ³n de entrenamiento
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ dataset/                 # Dataset principal
â”‚   â”‚   â”œâ”€â”€ train/images/        # 562 imÃ¡genes
â”‚   â”‚   â”œâ”€â”€ train/labels/        # Anotaciones YOLO
â”‚   â”‚   â”œâ”€â”€ val/images/
â”‚   â”‚   â””â”€â”€ val/labels/
â”‚   â”œâ”€â”€ pillar.yaml              # Config dataset YOLO
â”‚   â”œâ”€â”€ video.mp4                # Video de prueba
â”‚   â”œâ”€â”€ video_frames/            # Frames extraÃ­dos
â”‚   â””â”€â”€ templates/               # Templates para auto-anotaciÃ³n
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py                 # Entrenamiento completo
â”‚   â”œâ”€â”€ inference.py             # Inferencia (imagen/video/webcam)
â”‚   â”œâ”€â”€ auto_annotate.py         # Auto-anotaciÃ³n con template matching
â”‚   â”œâ”€â”€ visualize_annotations.py # Visualizar anotaciones
â”‚   â”œâ”€â”€ review_annotations.py    # Revisor interactivo de anotaciones
â”‚   â”œâ”€â”€ split_dataset.py         # Dividir train/val
â”‚   â”œâ”€â”€ evaluate.py              # EvaluaciÃ³n de mÃ©tricas
â”‚   â”œâ”€â”€ export_tensorrt.py       # ExportaciÃ³n a TensorRT/ONNX
â”‚   â””â”€â”€ benchmark.py             # Comparar velocidad de formatos
â”œâ”€â”€ models/                      # Modelos (.pt, .onnx, .engine)
â””â”€â”€ runs/                        # Logs de entrenamiento
```

---

## InstalaciÃ³n

```bash
# Crear entorno virtual
python3 -m venv .venv
source .venv/bin/activate

# Instalar dependencias
pip install -r requirements.txt

# Verificar CUDA
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"
```

---

## Uso

### Entrenamiento

```bash
source .venv/bin/activate
python scripts/train.py
```

ConfiguraciÃ³n en `config.yaml`:
- `epochs`: NÃºmero de Ã©pocas (30-100 recomendado)
- `batch`: TamaÃ±o de batch (8 para RTX 2060)
- `model`: Modelo base (`yolo12s.pt`)

### Inferencia

```bash
# Imagen
python scripts/inference.py --source imagen.jpg

# Video (usa TensorRT si estÃ¡ disponible)
python scripts/inference.py --source video.mp4 --model models/vr_boxes_best_20251207_171823.engine

# Webcam
python scripts/inference.py --source 0 --show

# Ajustar confianza (default: 0.65)
python scripts/inference.py --source video.mp4 --conf 0.5
```

### Exportar a TensorRT

```bash
# Exportar a TensorRT FP16 (2.5x mÃ¡s rÃ¡pido)
python scripts/export_tensorrt.py --format engine --half

# Exportar a ONNX
python scripts/export_tensorrt.py --format onnx
```

### Benchmark

```bash
python scripts/benchmark.py
```

---

## Pipeline de AnotaciÃ³n

### 1. Extraer frames de video

```python
import cv2
from pathlib import Path

video = cv2.VideoCapture('data/video.mp4')
fps = video.get(cv2.CAP_PROP_FPS)
frame_num = 0
saved = 0

while True:
    ret, frame = video.read()
    if not ret:
        break
    if frame_num % int(fps) == 0:  # 1 frame por segundo
        cv2.imwrite(f'data/frames/frame_{saved:04d}.jpg', frame)
        saved += 1
    frame_num += 1
video.release()
```

### 2. Auto-anotaciÃ³n con template matching

```bash
python scripts/auto_annotate.py \
  --frames data/frames/ \
  --template data/templates/pillar.jpg \
  --output data/labels/ \
  --threshold 0.7
```

**Limitaciones encontradas:**
- Template matching funciona bien con objetos distintivos
- Para patrones repetitivos (tablero amarillo/negro), genera falsos positivos
- Threshold 0.85-0.95 reduce falsos positivos pero puede perder detecciones

### 3. AnotaciÃ³n manual (cuando template matching falla)

Usamos **makesense.ai** (web, gratis):
1. Subir imÃ¡genes
2. Crear clase "pillar"
3. Dibujar bounding boxes
4. Exportar en formato YOLO

**Problema encontrado:** LabelImg crashea en Python 3.12 con PyQt5. Makesense.ai es la alternativa mÃ¡s estable.

### 4. Visualizar anotaciones

```bash
python scripts/visualize_annotations.py \
  --images data/frames/ \
  --labels data/labels/ \
  --output data/viz/ \
  --sample 10
```

---

## Problemas Encontrados y Soluciones

### 1. Template matching con falsos positivos

**Problema:** El patrÃ³n de tablero amarillo/negro es muy repetitivo, generando muchas detecciones falsas.

**SoluciÃ³n:**
- Aumentar threshold a 0.85-0.95
- Para pilares con patrÃ³n diferente (invertido), anotar manualmente
- Separar frames por tipo de pilar y procesar por separado

### 2. LabelImg crashea en Ubuntu/Python 3.12

**Error:** `TypeError: drawLine(): argument 1 has unexpected type 'float'`

**Causa:** Bug conocido de LabelImg con PyQt5 en Python 3.12

**SoluciÃ³n:** Usar makesense.ai (web) en su lugar

### 3. Pilares con patrÃ³n invertido no detectados

**Problema:** El modelo entrenado solo con pilares "amarillo arriba" no detecta pilares "negro arriba"

**SoluciÃ³n:**
1. Identificar frames con pilares invertidos (282-391)
2. Crear template especÃ­fico para patrÃ³n invertido
3. Auto-anotar esos frames
4. Re-entrenar con datos ampliados

### 4. Dataset con anotaciones vacÃ­as

**Problema:** 92 imÃ¡genes tenÃ­an archivos .txt pero vacÃ­os (sin anotaciones)

**DiagnÃ³stico:**
```bash
wc -l data/dataset/train/labels/frame_000300.txt
# Output: 0  (vacÃ­o)
```

**SoluciÃ³n:** Re-anotar esos frames especÃ­ficos

### 5. Entorno virtual desaparece

**Problema:** El .venv dejÃ³ de funcionar misteriosamente

**SoluciÃ³n:** Recrear:
```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### 6. SSH sin display grÃ¡fico

**Problema:** `--show` no funciona por SSH

**SoluciÃ³n:**
- El video se guarda en `runs/inference/`
- Descargar con `scp` o usar `ssh -X` para X11 forwarding

---

## Historial de Entrenamientos

### Entrenamiento 1: YOLOv8n, 10 Ã©pocas
- Dataset: Solo pilares normales
- Resultado: mAP50-95 = 62.7%

### Entrenamiento 2: YOLOv12s, 50 Ã©pocas
- Dataset: Solo pilares normales
- Resultado: mAP50-95 = 84.7%
- **Mejora significativa**

### Entrenamiento 3: YOLOv12s, 100 Ã©pocas
- Dataset: + 7 frames con pilares invertidos (manual)
- Resultado: mAP50-95 = 87.4%
- Los pilares invertidos siguen sin detectarse (pocos ejemplos)

### Entrenamiento 4: YOLOv12s, 30 Ã©pocas (en curso)
- Dataset: + 92 frames con pilares invertidos (auto-anotados)
- Objetivo: Detectar ambos tipos de pilares

---

## Comparativa YOLO8 vs YOLO12

| MÃ©trica | YOLOv8n (10 ep) | YOLOv12s (50 ep) |
|---------|-----------------|------------------|
| Precision | 90.4% | 93.3% |
| Recall | 93.0% | 97.5% |
| mAP50 | 96.4% | 97.9% |
| mAP50-95 | 62.7% | 84.7% |

**ConclusiÃ³n:** YOLOv12s con mÃ¡s Ã©pocas mejora significativamente la localizaciÃ³n (mAP50-95).

---

## Recursos de GPU (RTX 2060)

| OperaciÃ³n | VRAM | Potencia |
|-----------|------|----------|
| Idle | ~400MB | ~10W |
| Entrenamiento | ~3.5GB | ~115W |
| Inferencia | ~1.5GB | ~50W |

Temperatura tÃ­pica durante entrenamiento: 65-70Â°C

---

## Completado

- [x] Verificar detecciÃ³n de pilares invertidos tras entrenamiento 4
- [x] Exportar modelo a TensorRT para inferencia mÃ¡s rÃ¡pida (180 FPS)
- [x] Crear script de benchmark comparativo
- [x] AplicaciÃ³n web Gradio con todas las funcionalidades
- [x] DistribuciÃ³n Windows (install.bat / run.bat)

---

## Referencias

- [Ultralytics YOLO](https://docs.ultralytics.com/)
- [YOLOv12](https://docs.ultralytics.com/models/yolo12/)
- [makesense.ai](https://www.makesense.ai/) - AnotaciÃ³n web gratuita
- [Formato YOLO](https://docs.ultralytics.com/datasets/detect/)

---

*Ãšltima actualizaciÃ³n: Diciembre 2025*
