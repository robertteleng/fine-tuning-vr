{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Evaluación del Modelo\n",
    "\n",
    "Evaluación detallada del modelo entrenado.\n",
    "\n",
    "## Contenido\n",
    "1. Cargar modelo y dataset\n",
    "2. Ejecutar evaluación\n",
    "3. Analizar predicciones\n",
    "4. Visualizar errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "\n",
    "print(f\"Modelos disponibles:\")\n",
    "for m in MODELS_DIR.glob('*'):\n",
    "    size_mb = m.stat().st_size / (1024*1024)\n",
    "    print(f\"  - {m.name} ({size_mb:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cargar Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar mejor modelo .pt\n",
    "pt_models = sorted(MODELS_DIR.glob('*.pt'), reverse=True)\n",
    "if pt_models:\n",
    "    model_path = pt_models[0]\n",
    "    print(f\"Cargando: {model_path.name}\")\n",
    "    model = YOLO(str(model_path))\n",
    "    print(f\"Modelo cargado correctamente\")\n",
    "else:\n",
    "    print(\"No se encontraron modelos .pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ejecutar Evaluación en Validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar en dataset de validación\n",
    "data_yaml = DATA_DIR / 'pillar.yaml'\n",
    "\n",
    "results = model.val(\n",
    "    data=str(data_yaml),\n",
    "    split='val',\n",
    "    conf=0.001,  # Bajo para evaluación completa\n",
    "    iou=0.6,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar métricas\n",
    "print(\"=\"*50)\n",
    "print(\"MÉTRICAS DE EVALUACIÓN\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Precision:  {results.box.mp:.4f} ({results.box.mp*100:.1f}%)\")\n",
    "print(f\"Recall:     {results.box.mr:.4f} ({results.box.mr*100:.1f}%)\")\n",
    "print(f\"mAP@50:     {results.box.map50:.4f} ({results.box.map50*100:.1f}%)\")\n",
    "print(f\"mAP@50-95:  {results.box.map:.4f} ({results.box.map*100:.1f}%)\")\n",
    "\n",
    "# F1 Score\n",
    "if results.box.mp > 0 and results.box.mr > 0:\n",
    "    f1 = 2 * (results.box.mp * results.box.mr) / (results.box.mp + results.box.mr)\n",
    "    print(f\"F1 Score:   {f1:.4f} ({f1*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analizar Predicciones por Confianza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener predicciones en algunas imágenes\n",
    "val_images = list((DATA_DIR / 'dataset' / 'val' / 'images').glob('*.jpg'))[:20]\n",
    "\n",
    "all_confidences = []\n",
    "for img_path in val_images:\n",
    "    results = model(str(img_path), verbose=False)\n",
    "    if len(results[0].boxes) > 0:\n",
    "        confs = results[0].boxes.conf.cpu().numpy()\n",
    "        all_confidences.extend(confs)\n",
    "\n",
    "if all_confidences:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.hist(all_confidences, bins=20, edgecolor='black', alpha=0.7)\n",
    "    plt.axvline(x=0.65, color='r', linestyle='--', label='Threshold (0.65)')\n",
    "    plt.xlabel('Confianza')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.title('Distribución de Confianza en Predicciones')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nEstadísticas de confianza:\")\n",
    "    print(f\"  Min: {min(all_confidences):.3f}\")\n",
    "    print(f\"  Max: {max(all_confidences):.3f}\")\n",
    "    print(f\"  Media: {np.mean(all_confidences):.3f}\")\n",
    "    print(f\"  Mediana: {np.median(all_confidences):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizar Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(img_path, conf_threshold=0.65):\n",
    "    \"\"\"Muestra predicciones en una imagen.\"\"\"\n",
    "    results = model(str(img_path), conf=conf_threshold, verbose=False)\n",
    "    \n",
    "    # Imagen con anotaciones\n",
    "    annotated = results[0].plot()\n",
    "    annotated = cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(annotated)\n",
    "    plt.title(f\"{img_path.name} - {len(results[0].boxes)} detecciones\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    return results[0]\n",
    "\n",
    "# Mostrar algunas predicciones\n",
    "sample_images = val_images[:4]\n",
    "for img_path in sample_images:\n",
    "    show_predictions(img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Análisis de Errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_annotations(label_path):\n",
    "    \"\"\"Cuenta anotaciones en un archivo YOLO.\"\"\"\n",
    "    if not label_path.exists():\n",
    "        return 0\n",
    "    with open(label_path) as f:\n",
    "        return len([l for l in f if l.strip()])\n",
    "\n",
    "# Comparar predicciones vs ground truth\n",
    "errors = {'false_positives': [], 'false_negatives': [], 'correct': []}\n",
    "\n",
    "for img_path in val_images:\n",
    "    label_path = img_path.parent.parent / 'labels' / f\"{img_path.stem}.txt\"\n",
    "    gt_count = count_annotations(label_path)\n",
    "    \n",
    "    results = model(str(img_path), conf=0.65, verbose=False)\n",
    "    pred_count = len(results[0].boxes)\n",
    "    \n",
    "    diff = pred_count - gt_count\n",
    "    if diff > 0:\n",
    "        errors['false_positives'].append((img_path.name, diff))\n",
    "    elif diff < 0:\n",
    "        errors['false_negatives'].append((img_path.name, -diff))\n",
    "    else:\n",
    "        errors['correct'].append(img_path.name)\n",
    "\n",
    "print(f\"Análisis de {len(val_images)} imágenes:\")\n",
    "print(f\"  Correctas: {len(errors['correct'])}\")\n",
    "print(f\"  Con falsos positivos: {len(errors['false_positives'])}\")\n",
    "print(f\"  Con falsos negativos: {len(errors['false_negatives'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar ejemplos de errores\n",
    "if errors['false_negatives']:\n",
    "    print(\"\\nImágenes con detecciones faltantes:\")\n",
    "    for name, count in errors['false_negatives'][:5]:\n",
    "        print(f\"  {name}: {count} faltantes\")\n",
    "\n",
    "if errors['false_positives']:\n",
    "    print(\"\\nImágenes con detecciones extra:\")\n",
    "    for name, count in errors['false_positives'][:5]:\n",
    "        print(f\"  {name}: {count} extra\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Velocidad de Inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "# Warmup\n",
    "dummy = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)\n",
    "for _ in range(5):\n",
    "    model(dummy, verbose=False)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "# Benchmark\n",
    "times = []\n",
    "for _ in range(50):\n",
    "    start = time.perf_counter()\n",
    "    model(dummy, verbose=False)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    times.append(time.perf_counter() - start)\n",
    "\n",
    "mean_ms = np.mean(times) * 1000\n",
    "fps = 1000 / mean_ms\n",
    "\n",
    "print(f\"Velocidad de inferencia ({model_path.name}):\")\n",
    "print(f\"  Media: {mean_ms:.2f} ms\")\n",
    "print(f\"  FPS: {fps:.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
