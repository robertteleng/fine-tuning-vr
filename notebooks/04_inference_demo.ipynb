{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Demo de Inferencia Interactivo\n",
    "\n",
    "Demo interactivo para probar el modelo con diferentes configuraciones.\n",
    "\n",
    "## Contenido\n",
    "1. Cargar modelo\n",
    "2. Inferencia en imagen con widgets\n",
    "3. Comparar formatos (PyTorch vs TensorRT)\n",
    "4. Procesar video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "\n",
    "print(\"VR Pillar Detector - Demo Interactivo\")\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cargar Modelos Disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar todos los modelos\n",
    "available_models = {}\n",
    "\n",
    "for ext, name in [('.engine', 'TensorRT'), ('.pt', 'PyTorch'), ('.onnx', 'ONNX')]:\n",
    "    models = list(MODELS_DIR.glob(f'*{ext}'))\n",
    "    if models:\n",
    "        model_path = models[0]\n",
    "        available_models[name] = model_path\n",
    "        size_mb = model_path.stat().st_size / (1024*1024)\n",
    "        print(f\"{name}: {model_path.name} ({size_mb:.1f} MB)\")\n",
    "\n",
    "# Cargar modelo por defecto (TensorRT si existe, sino PyTorch)\n",
    "default_format = 'TensorRT' if 'TensorRT' in available_models else 'PyTorch'\n",
    "model = YOLO(str(available_models[default_format]))\n",
    "print(f\"\\nModelo cargado: {default_format}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inferencia Interactiva en Imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener imágenes de ejemplo\n",
    "sample_images = list((DATA_DIR / 'dataset' / 'val' / 'images').glob('*.jpg'))[:20]\n",
    "image_names = [img.name for img in sample_images]\n",
    "\n",
    "# Widgets\n",
    "image_dropdown = widgets.Dropdown(\n",
    "    options=image_names,\n",
    "    value=image_names[0] if image_names else None,\n",
    "    description='Imagen:'\n",
    ")\n",
    "\n",
    "conf_slider = widgets.FloatSlider(\n",
    "    value=0.65,\n",
    "    min=0.1,\n",
    "    max=1.0,\n",
    "    step=0.05,\n",
    "    description='Confianza:'\n",
    ")\n",
    "\n",
    "iou_slider = widgets.FloatSlider(\n",
    "    value=0.45,\n",
    "    min=0.1,\n",
    "    max=1.0,\n",
    "    step=0.05,\n",
    "    description='IoU:'\n",
    ")\n",
    "\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=list(available_models.keys()),\n",
    "    value=default_format,\n",
    "    description='Modelo:'\n",
    ")\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "def run_inference(image_name, conf, iou, model_format):\n",
    "    global model\n",
    "    \n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        # Cargar modelo si cambió\n",
    "        if model_format in available_models:\n",
    "            model = YOLO(str(available_models[model_format]))\n",
    "        \n",
    "        # Encontrar imagen\n",
    "        img_path = DATA_DIR / 'dataset' / 'val' / 'images' / image_name\n",
    "        if not img_path.exists():\n",
    "            print(f\"Imagen no encontrada: {img_path}\")\n",
    "            return\n",
    "        \n",
    "        # Inferencia\n",
    "        import time\n",
    "        start = time.perf_counter()\n",
    "        results = model(str(img_path), conf=conf, iou=iou, verbose=False)\n",
    "        elapsed = (time.perf_counter() - start) * 1000\n",
    "        \n",
    "        # Mostrar resultado\n",
    "        annotated = results[0].plot()\n",
    "        annotated = cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        plt.figure(figsize=(14, 8))\n",
    "        plt.imshow(annotated)\n",
    "        plt.title(f\"{image_name} | {len(results[0].boxes)} detecciones | {elapsed:.1f}ms | {model_format}\")\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Detalles\n",
    "        if len(results[0].boxes) > 0:\n",
    "            print(f\"\\nDetecciones:\")\n",
    "            for i, box in enumerate(results[0].boxes):\n",
    "                conf_val = box.conf.item()\n",
    "                print(f\"  [{i}] Confianza: {conf_val:.3f}\")\n",
    "\n",
    "# Crear interfaz interactiva\n",
    "ui = widgets.VBox([\n",
    "    widgets.HBox([image_dropdown, model_dropdown]),\n",
    "    widgets.HBox([conf_slider, iou_slider]),\n",
    "    output\n",
    "])\n",
    "\n",
    "# Conectar eventos\n",
    "def on_change(change):\n",
    "    run_inference(image_dropdown.value, conf_slider.value, iou_slider.value, model_dropdown.value)\n",
    "\n",
    "image_dropdown.observe(on_change, names='value')\n",
    "conf_slider.observe(on_change, names='value')\n",
    "iou_slider.observe(on_change, names='value')\n",
    "model_dropdown.observe(on_change, names='value')\n",
    "\n",
    "display(ui)\n",
    "\n",
    "# Ejecutar inicial\n",
    "run_inference(image_dropdown.value, conf_slider.value, iou_slider.value, model_dropdown.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparar Velocidad de Formatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "def benchmark_model(model_path, iterations=50):\n",
    "    \"\"\"Benchmark de velocidad.\"\"\"\n",
    "    model = YOLO(str(model_path))\n",
    "    dummy = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        model(dummy, verbose=False)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    for _ in range(iterations):\n",
    "        start = time.perf_counter()\n",
    "        model(dummy, verbose=False)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        times.append(time.perf_counter() - start)\n",
    "    \n",
    "    return np.mean(times) * 1000\n",
    "\n",
    "# Comparar todos los formatos\n",
    "print(\"Benchmark de velocidad (50 iteraciones):\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "results = {}\n",
    "for name, path in available_models.items():\n",
    "    try:\n",
    "        mean_ms = benchmark_model(path)\n",
    "        fps = 1000 / mean_ms\n",
    "        results[name] = {'ms': mean_ms, 'fps': fps}\n",
    "        print(f\"{name:12} {mean_ms:>8.2f} ms  ({fps:>6.1f} FPS)\")\n",
    "    except Exception as e:\n",
    "        print(f\"{name:12} Error: {e}\")\n",
    "\n",
    "# Gráfico\n",
    "if results:\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    names = list(results.keys())\n",
    "    fps_values = [results[n]['fps'] for n in names]\n",
    "    \n",
    "    bars = ax.bar(names, fps_values, color=['#2ecc71', '#3498db', '#e74c3c'][:len(names)])\n",
    "    ax.set_ylabel('FPS')\n",
    "    ax.set_title('Comparativa de Velocidad por Formato')\n",
    "    \n",
    "    for bar, fps in zip(bars, fps_values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n",
    "                f'{fps:.0f}', ha='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Procesar Video (muestra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar video de prueba\n",
    "video_path = DATA_DIR / 'video.mp4'\n",
    "\n",
    "if video_path.exists():\n",
    "    print(f\"Video encontrado: {video_path}\")\n",
    "    \n",
    "    # Procesar primeros N frames\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    print(f\"FPS: {fps}, Frames totales: {total_frames}\")\n",
    "    \n",
    "    # Mostrar algunos frames procesados\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    frame_indices = np.linspace(0, total_frames-1, 6, dtype=int)\n",
    "    \n",
    "    for ax, frame_idx in zip(axes, frame_indices):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            results = model(frame, conf=0.65, verbose=False)\n",
    "            annotated = results[0].plot()\n",
    "            annotated = cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            ax.imshow(annotated)\n",
    "            ax.set_title(f\"Frame {frame_idx} - {len(results[0].boxes)} det.\")\n",
    "            ax.axis('off')\n",
    "    \n",
    "    cap.release()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Video no encontrado en: {video_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Subir tu propia imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Widget para subir imagen\n",
    "upload = widgets.FileUpload(\n",
    "    accept='image/*',\n",
    "    multiple=False,\n",
    "    description='Subir imagen'\n",
    ")\n",
    "\n",
    "upload_output = widgets.Output()\n",
    "\n",
    "def on_upload(change):\n",
    "    with upload_output:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        if upload.value:\n",
    "            # Obtener imagen\n",
    "            uploaded_file = list(upload.value.values())[0]\n",
    "            content = uploaded_file['content']\n",
    "            \n",
    "            # Convertir a numpy\n",
    "            import io\n",
    "            img = Image.open(io.BytesIO(content))\n",
    "            img_np = np.array(img)\n",
    "            \n",
    "            # Inferencia\n",
    "            results = model(img_np, conf=0.65, verbose=False)\n",
    "            annotated = results[0].plot()\n",
    "            \n",
    "            if len(annotated.shape) == 3 and annotated.shape[2] == 3:\n",
    "                annotated = cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            plt.figure(figsize=(14, 8))\n",
    "            plt.imshow(annotated)\n",
    "            plt.title(f\"Tu imagen - {len(results[0].boxes)} detecciones\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "upload.observe(on_upload, names='value')\n",
    "\n",
    "display(widgets.VBox([upload, upload_output]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
