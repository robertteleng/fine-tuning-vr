{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Grounding DINO: Auto-Anotaci√≥n Zero-Shot\n",
    "\n",
    "Demo interactivo de anotaci√≥n autom√°tica usando lenguaje natural.\n",
    "\n",
    "## ¬øQu√© es Grounding DINO?\n",
    "- Modelo que detecta objetos a partir de **descripciones de texto**\n",
    "- No necesita entrenamiento previo (zero-shot)\n",
    "- Ideal para crear datasets r√°pidamente\n",
    "\n",
    "## Contenido\n",
    "1. Instalaci√≥n y carga del modelo\n",
    "2. Detecci√≥n con prompt simple\n",
    "3. Experimentar con diferentes prompts\n",
    "4. Generar anotaciones YOLO\n",
    "5. Comparativa: YOLO-World vs Grounding DINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cargar Grounding DINO\n",
    "\n",
    "Usamos el modelo de HuggingFace que es f√°cil de instalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar si no est√° (descomentar si es necesario)\n",
    "# !pip install transformers\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "\n",
    "MODEL_NAME = \"IDEA-Research/grounding-dino-tiny\"\n",
    "\n",
    "print(f\"Cargando modelo: {MODEL_NAME}\")\n",
    "print(\"(Primera vez descarga ~400MB)\\n\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(MODEL_NAME)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"‚úÖ Modelo cargado en: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Detecci√≥n con Prompt Simple\n",
    "\n",
    "Probemos con una imagen de ejemplo y el prompt para pilares VR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_objects(image_path, prompt, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Detecta objetos en una imagen usando un prompt de texto.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Ruta a la imagen\n",
    "        prompt: Descripci√≥n del objeto (ej: \"yellow and black checkered box\")\n",
    "        threshold: Confianza m√≠nima (0.0-1.0)\n",
    "    \n",
    "    Returns:\n",
    "        boxes: Coordenadas [x1, y1, x2, y2]\n",
    "        scores: Confianzas\n",
    "    \"\"\"\n",
    "    # Cargar imagen\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    w, h = image.size\n",
    "    \n",
    "    # IMPORTANTE: Grounding DINO requiere punto final en el prompt\n",
    "    prompt_with_dot = prompt if prompt.endswith(\".\") else prompt + \".\"\n",
    "    \n",
    "    # Procesar\n",
    "    inputs = processor(images=image, text=prompt_with_dot, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Inferencia\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Post-procesar\n",
    "    results = processor.post_process_grounded_object_detection(\n",
    "        outputs,\n",
    "        inputs[\"input_ids\"],\n",
    "        box_threshold=threshold,\n",
    "        text_threshold=threshold,\n",
    "        target_sizes=[(h, w)]\n",
    "    )[0]\n",
    "    \n",
    "    return results[\"boxes\"].cpu().numpy(), results[\"scores\"].cpu().numpy(), image\n",
    "\n",
    "print(\"Funci√≥n detect_objects() definida ‚úÖ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detections(image, boxes, scores, title=\"Detecciones\"):\n",
    "    \"\"\"\n",
    "    Visualiza las detecciones sobre la imagen.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    for box, score in zip(boxes, scores):\n",
    "        x1, y1, x2, y2 = box\n",
    "        w, h = x2 - x1, y2 - y1\n",
    "        \n",
    "        # Rect√°ngulo\n",
    "        rect = patches.Rectangle(\n",
    "            (x1, y1), w, h,\n",
    "            linewidth=2, edgecolor='lime', facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Etiqueta con confianza\n",
    "        ax.text(x1, y1 - 5, f\"{score:.2f}\", color='white', fontsize=10,\n",
    "               fontweight='bold', bbox=dict(boxstyle='round', facecolor='lime', alpha=0.8))\n",
    "    \n",
    "    ax.set_title(f\"{title} - {len(boxes)} detecciones\")\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return len(boxes)\n",
    "\n",
    "print(\"Funci√≥n visualize_detections() definida ‚úÖ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar imagen de ejemplo\n",
    "sample_images = list((DATA_DIR / 'dataset' / 'val' / 'images').glob('*.jpg'))\n",
    "if not sample_images:\n",
    "    sample_images = list((DATA_DIR / 'video_frames').glob('*.jpg'))\n",
    "\n",
    "if sample_images:\n",
    "    test_image = sample_images[0]\n",
    "    print(f\"Imagen de prueba: {test_image.name}\")\n",
    "    \n",
    "    # Detectar con prompt\n",
    "    PROMPT = \"yellow and black checkered box\"\n",
    "    print(f\"Prompt: '{PROMPT}'\")\n",
    "    print(f\"Threshold: 0.3\\n\")\n",
    "    \n",
    "    boxes, scores, image = detect_objects(test_image, PROMPT, threshold=0.3)\n",
    "    visualize_detections(image, boxes, scores, title=f\"Prompt: '{PROMPT}'\")\n",
    "    \n",
    "    # Mostrar estad√≠sticas\n",
    "    if len(scores) > 0:\n",
    "        print(f\"\\nEstad√≠sticas:\")\n",
    "        print(f\"  Detecciones: {len(scores)}\")\n",
    "        print(f\"  Confianza min: {scores.min():.3f}\")\n",
    "        print(f\"  Confianza max: {scores.max():.3f}\")\n",
    "        print(f\"  Confianza media: {scores.mean():.3f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No se encontraron im√°genes de ejemplo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experimentar con Diferentes Prompts\n",
    "\n",
    "El prompt es **clave** para buenos resultados. Probemos variaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diferentes prompts para probar\n",
    "PROMPTS_TO_TEST = [\n",
    "    \"yellow and black checkered box\",\n",
    "    \"checkered pillar\",\n",
    "    \"yellow black striped cube\",\n",
    "    \"warning pillar\",\n",
    "    \"box\",  # Muy gen√©rico\n",
    "]\n",
    "\n",
    "if sample_images:\n",
    "    test_image = sample_images[0]\n",
    "    \n",
    "    print(\"Comparando prompts:\\n\")\n",
    "    print(f\"{'Prompt':<40} {'Detecciones':>12} {'Conf. Media':>12}\")\n",
    "    print(\"=\"*66)\n",
    "    \n",
    "    results_by_prompt = {}\n",
    "    \n",
    "    for prompt in PROMPTS_TO_TEST:\n",
    "        boxes, scores, _ = detect_objects(test_image, prompt, threshold=0.25)\n",
    "        \n",
    "        n_det = len(scores)\n",
    "        mean_conf = scores.mean() if n_det > 0 else 0\n",
    "        \n",
    "        results_by_prompt[prompt] = {'count': n_det, 'mean_conf': mean_conf}\n",
    "        print(f\"{prompt:<40} {n_det:>12} {mean_conf:>12.3f}\")\n",
    "    \n",
    "    # Recomendar mejor prompt\n",
    "    best = max(results_by_prompt.items(), key=lambda x: (x[1]['count'], x[1]['mean_conf']))\n",
    "    print(f\"\\n‚úÖ Mejor prompt: '{best[0]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar el mejor prompt vs uno gen√©rico\n",
    "if sample_images:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    prompts_compare = [\"yellow and black checkered box\", \"box\"]\n",
    "    \n",
    "    for ax, prompt in zip(axes, prompts_compare):\n",
    "        boxes, scores, image = detect_objects(test_image, prompt, threshold=0.25)\n",
    "        \n",
    "        ax.imshow(image)\n",
    "        for box, score in zip(boxes, scores):\n",
    "            x1, y1, x2, y2 = box\n",
    "            rect = patches.Rectangle(\n",
    "                (x1, y1), x2-x1, y2-y1,\n",
    "                linewidth=2, edgecolor='lime', facecolor='none'\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(x1, y1-5, f\"{score:.2f}\", color='white', fontsize=9,\n",
    "                   bbox=dict(facecolor='lime', alpha=0.7))\n",
    "        \n",
    "        ax.set_title(f\"'{prompt}'\\n{len(boxes)} detecciones\")\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(\"Espec√≠fico vs Gen√©rico\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generar Anotaciones YOLO\n",
    "\n",
    "Convertir las detecciones al formato YOLO para entrenar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxes_to_yolo(boxes, img_width, img_height, class_id=0):\n",
    "    \"\"\"\n",
    "    Convierte boxes [x1,y1,x2,y2] a formato YOLO normalizado.\n",
    "    \n",
    "    YOLO format: class x_center y_center width height (todos en [0,1])\n",
    "    \"\"\"\n",
    "    yolo_lines = []\n",
    "    \n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = box\n",
    "        \n",
    "        # Centro\n",
    "        x_center = (x1 + x2) / 2 / img_width\n",
    "        y_center = (y1 + y2) / 2 / img_height\n",
    "        \n",
    "        # Tama√±o\n",
    "        width = (x2 - x1) / img_width\n",
    "        height = (y2 - y1) / img_height\n",
    "        \n",
    "        # Clamp a [0,1]\n",
    "        x_center = max(0, min(1, x_center))\n",
    "        y_center = max(0, min(1, y_center))\n",
    "        width = max(0, min(1, width))\n",
    "        height = max(0, min(1, height))\n",
    "        \n",
    "        yolo_lines.append(f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\")\n",
    "    \n",
    "    return yolo_lines\n",
    "\n",
    "# Ejemplo\n",
    "if sample_images:\n",
    "    boxes, scores, image = detect_objects(test_image, \"yellow and black checkered box\", 0.3)\n",
    "    w, h = image.size\n",
    "    \n",
    "    yolo_lines = boxes_to_yolo(boxes, w, h, class_id=0)\n",
    "    \n",
    "    print(\"Formato YOLO generado:\")\n",
    "    print(\"-\" * 60)\n",
    "    for line in yolo_lines:\n",
    "        print(line)\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"\\nGuardar en: {test_image.stem}.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_batch(image_paths, prompt, output_dir, threshold=0.3, class_id=0):\n",
    "    \"\"\"\n",
    "    Anota un lote de im√°genes y guarda labels YOLO.\n",
    "    \"\"\"\n",
    "    from tqdm.notebook import tqdm\n",
    "    \n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    stats = {'total': 0, 'with_detections': 0, 'total_boxes': 0}\n",
    "    \n",
    "    for img_path in tqdm(image_paths, desc=\"Anotando\"):\n",
    "        try:\n",
    "            boxes, scores, image = detect_objects(img_path, prompt, threshold)\n",
    "            w, h = image.size\n",
    "            \n",
    "            yolo_lines = boxes_to_yolo(boxes, w, h, class_id)\n",
    "            \n",
    "            # Guardar label\n",
    "            label_path = output_dir / f\"{img_path.stem}.txt\"\n",
    "            with open(label_path, 'w') as f:\n",
    "                f.write('\\n'.join(yolo_lines))\n",
    "            \n",
    "            stats['total'] += 1\n",
    "            if len(boxes) > 0:\n",
    "                stats['with_detections'] += 1\n",
    "                stats['total_boxes'] += len(boxes)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error en {img_path.name}: {e}\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "print(\"Funci√≥n annotate_batch() definida ‚úÖ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: anotar algunas im√°genes\n",
    "if sample_images:\n",
    "    demo_images = sample_images[:5]  # Solo 5 para demo\n",
    "    output_demo = PROJECT_ROOT / 'data' / 'demo_labels'\n",
    "    \n",
    "    print(f\"Anotando {len(demo_images)} im√°genes de demo...\\n\")\n",
    "    \n",
    "    stats = annotate_batch(\n",
    "        demo_images,\n",
    "        prompt=\"yellow and black checkered box\",\n",
    "        output_dir=output_demo,\n",
    "        threshold=0.3\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Resultados:\")\n",
    "    print(f\"   Im√°genes procesadas: {stats['total']}\")\n",
    "    print(f\"   Con detecciones: {stats['with_detections']}\")\n",
    "    print(f\"   Total boxes: {stats['total_boxes']}\")\n",
    "    print(f\"   Promedio por imagen: {stats['total_boxes']/stats['total']:.1f}\")\n",
    "    print(f\"\\n   Labels guardados en: {output_demo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparativa: YOLO-World vs Grounding DINO\n",
    "\n",
    "¬øPor qu√© elegimos Grounding DINO?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intentar cargar YOLO-World para comparar\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "    \n",
    "    print(\"Cargando YOLO-World...\")\n",
    "    yolo_world = YOLO('yolov8s-worldv2.pt')\n",
    "    yolo_world.set_classes([\"yellow and black checkered box\"])\n",
    "    \n",
    "    HAS_YOLO_WORLD = True\n",
    "    print(\"‚úÖ YOLO-World cargado\")\n",
    "except Exception as e:\n",
    "    HAS_YOLO_WORLD = False\n",
    "    print(f\"‚ö†Ô∏è YOLO-World no disponible: {e}\")\n",
    "    print(\"   Comparativa solo mostrar√° resultados previos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla comparativa de resultados previos (del DIARY.md)\n",
    "import pandas as pd\n",
    "\n",
    "comparison_data = {\n",
    "    'Modelo': ['YOLO-World', 'Grounding DINO'],\n",
    "    'Prompt': ['yellow black checkered cube', 'yellow and black checkered box'],\n",
    "    'Cobertura': ['8.5%', '100%'],\n",
    "    'Detecciones (47 imgs)': [6, 259],\n",
    "    'Confianza': ['0.05-0.09', '0.25-0.69'],\n",
    "    'Velocidad': ['~25 img/s', '~2 img/s'],\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARATIVA: YOLO-World vs Grounding DINO\")\n",
    "print(\"=\"*70)\n",
    "display(df_comparison)\n",
    "\n",
    "print(\"\\nüí° Conclusi√≥n:\")\n",
    "print(\"   - YOLO-World es m√°s r√°pido pero no detecta objetos VR espec√≠ficos\")\n",
    "print(\"   - Grounding DINO es 43x mejor para nuestro caso de uso\")\n",
    "print(\"   - CLIP (base de YOLO-World) fue entrenado con fotos reales, no VR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°fico comparativo\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Detecciones\n",
    "ax1 = axes[0]\n",
    "models = ['YOLO-World', 'Grounding DINO']\n",
    "detections = [6, 259]\n",
    "colors = ['#e74c3c', '#2ecc71']\n",
    "bars = ax1.bar(models, detections, color=colors, edgecolor='black')\n",
    "ax1.set_ylabel('Detecciones')\n",
    "ax1.set_title('Total Detecciones (47 im√°genes)')\n",
    "for bar, det in zip(bars, detections):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n",
    "             str(det), ha='center', fontweight='bold')\n",
    "\n",
    "# Cobertura\n",
    "ax2 = axes[1]\n",
    "coverage = [8.5, 100]\n",
    "bars = ax2.bar(models, coverage, color=colors, edgecolor='black')\n",
    "ax2.set_ylabel('Cobertura (%)')\n",
    "ax2.set_title('Im√°genes con Detecciones')\n",
    "ax2.set_ylim(0, 110)\n",
    "for bar, cov in zip(bars, coverage):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,\n",
    "             f\"{cov}%\", ha='center', fontweight='bold')\n",
    "\n",
    "plt.suptitle('¬øPor qu√© Grounding DINO?', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tips para Mejores Resultados\n",
    "\n",
    "### Gu√≠a de Prompts\n",
    "\n",
    "| Tipo | Ejemplo | Cu√°ndo usar |\n",
    "|------|---------|-------------|\n",
    "| Descriptivo | \"yellow and black checkered box\" | Objeto con patr√≥n espec√≠fico |\n",
    "| Material | \"wooden crate\" | Textura distintiva |\n",
    "| Funci√≥n | \"warning pillar\" | Objeto con prop√≥sito conocido |\n",
    "| Color | \"red cube\" | Color es la caracter√≠stica principal |\n",
    "\n",
    "### Ajuste de Threshold\n",
    "\n",
    "| Threshold | Comportamiento |\n",
    "|-----------|----------------|\n",
    "| 0.1 - 0.2 | Muchas detecciones, posibles falsos positivos |\n",
    "| 0.25 - 0.35 | **Recomendado** para anotaci√≥n inicial |\n",
    "| 0.4 - 0.5 | M√°s conservador, puede perder objetos |\n",
    "| 0.6+ | Muy estricto, solo detecciones muy seguras |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efecto del threshold\n",
    "if sample_images:\n",
    "    thresholds = [0.15, 0.25, 0.35, 0.50]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    \n",
    "    for ax, thresh in zip(axes, thresholds):\n",
    "        boxes, scores, image = detect_objects(test_image, \"yellow and black checkered box\", thresh)\n",
    "        \n",
    "        ax.imshow(image)\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = box\n",
    "            rect = patches.Rectangle(\n",
    "                (x1, y1), x2-x1, y2-y1,\n",
    "                linewidth=2, edgecolor='lime', facecolor='none'\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "        \n",
    "        ax.set_title(f\"Threshold: {thresh}\\n{len(boxes)} detecciones\")\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('Efecto del Threshold en las Detecciones', fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Limpieza\n",
    "\n",
    "Eliminar archivos de demo si se crearon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar carpeta demo (opcional)\n",
    "import shutil\n",
    "\n",
    "demo_dir = PROJECT_ROOT / 'data' / 'demo_labels'\n",
    "if demo_dir.exists():\n",
    "    response = input(\"¬øEliminar carpeta demo_labels? (s/n): \")\n",
    "    if response.lower() == 's':\n",
    "        shutil.rmtree(demo_dir)\n",
    "        print(\"‚úÖ Carpeta demo_labels eliminada\")\n",
    "    else:\n",
    "        print(\"Carpeta conservada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pr√≥ximos Pasos\n",
    "\n",
    "1. **Usar el script completo** para anotar todo un dataset:\n",
    "   ```bash\n",
    "   python scripts/auto_annotate_grounding_dino.py \\\n",
    "       --source data/video_frames/ \\\n",
    "       --prompt \"yellow and black checkered box\" \\\n",
    "       --output data/dataset_auto/ \\\n",
    "       --threshold 0.3\n",
    "   ```\n",
    "\n",
    "2. **Revisar anotaciones** con el Annotation Reviewer en la GUI:\n",
    "   ```bash\n",
    "   python app.py\n",
    "   # Tab: Annotations\n",
    "   ```\n",
    "\n",
    "3. **Entrenar modelo** con el nuevo dataset:\n",
    "   ```bash\n",
    "   python scripts/train.py\n",
    "   ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
